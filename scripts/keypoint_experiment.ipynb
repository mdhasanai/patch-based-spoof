{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keypoint based experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value_checker_ica.py', 'deeppix_customized.py', 'visualize_five.py', 'models', 'filter-analysis-ica.ipynb', 'main-ica.py', 'fine_tune.py', 'patch_based_heatmap_maker.py', 'keypoint_experiment.ipynb', 'analytics', 'infer.ipynb', 'config.py', 'patch_model.py', 'all_patch_train.py', 'config.json', 'patch_train.py', 'remove_damaged_images.py', '__pycache__', 'spoof-heatmap.png', 'eval', 'all_keypoin.py', 'facerec', 'deeppix_train.py', 'Untitled.ipynb', 'combined-heatmap.png', 'ica_filters.py', 'infer_deeppix.py', '__init__.py', 'live-heatmap.png', 'metrics', '.ipynb_checkpoints', 'middle_patch', 'patch_dicts', 'testpoint.py', 'explore_the_implementation.ipynb', 'ica_train.py', 'spoofv4.pt', 'eval_keypoint.py', 'gaze_data_maker.py', 'focal_loss.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "sys.path.append(path.join(path.dirname(\"eval_keypoint.py\"), '..'))\n",
    "sys.path.append(path.join(path.dirname(\"eval_keypoint.py\"), '../..'))\n",
    "\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from skimage.util.shape import view_as_windows\n",
    "from facerec.retinaface_detection import RetinaDetector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ids= [i for i in range(0, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class New_PatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None, color_mode=['rgb'], im_size=96, patch_size=None, phase='train', training_type='None'):\n",
    "        self.fnames = path\n",
    "        self.color_mode = color_mode\n",
    "        self.im_size = im_size\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.phase = phase\n",
    "        self.training_type = training_type\n",
    "        self.detector = RetinaDetector()\n",
    "        \n",
    "        self.count = 0\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im, label = self.create_single_sample(idx)\n",
    "    \n",
    "        return im, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "        \n",
    "    def create_single_sample(self, idx):\n",
    "        \n",
    "        im_path = self.fnames[idx]\n",
    "\n",
    "        label = self.fnames[idx].split(\"/\")#[-2]\n",
    "\n",
    "        img = cv2.imread(im_path)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "\n",
    "        imgs = {}\n",
    "        for mode in self.color_mode:\n",
    "            imgs[mode] = cv2.cvtColor(img, eval(f'cv2.COLOR_BGR2{mode.upper()}'))\n",
    "        \n",
    "        imgs = self.getting_patches(imgs, im_path)\n",
    "        if \"spoof\" in label or \"device\" in label or \"print\" in label or \"print2\" in label or \"video-replay2\" in label:\n",
    "            ground = torch.ones(1)\n",
    "        elif \"print1\" in label or \"video-replay1\" in label:\n",
    "            ground = torch.ones(1)\n",
    "        elif \"live\" in label or \"real\" in label:\n",
    "            ground = torch.zeros(1)\n",
    "#         else:\n",
    "#             print(label)\n",
    "        return imgs, ground\n",
    "    \n",
    "    def getting_patches(self, img_patch, im_path_name):\n",
    "\n",
    "        \n",
    "        new_im = img_patch['rgb']\n",
    "        \n",
    "        y1,y2,x1,x2 = self.getting_middle_path(im_path_name)\n",
    "        new_im = new_im[y1:y2,x1:x2,:]\n",
    "        new_im_m_tr = self.transform(new_im)\n",
    "        \n",
    "        return new_im_m_tr\n",
    "    \n",
    "\n",
    "    def divide_single_img_into_patches(self, img, size=(224, 224), patch_size=(48, 48, 3), step=1):\n",
    "\n",
    "        img = cv2.resize(img, size)\n",
    "        #             print(img.shape, len(patch_size))\n",
    "        patch_grid = view_as_windows(img, patch_size, step)\n",
    "        #         print(patch_grid.shape)\n",
    "        return patch_grid\n",
    "    \n",
    "    def get_another_landmark(self):\n",
    "        img = np.array([[[ 73.25691 , 106.048355],\n",
    "                        [149.7507  , 105.70673 ],\n",
    "                        [111.53898 , 141.779   ],\n",
    "                        [ 79.02094 , 186.59473 ],\n",
    "                        [141.23201 , 186.73901 ]]], dtype=np.float32)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def getting_middle_path(self,im_path):\n",
    "        \"\"\"\n",
    "            landmark 0: eye-1     (x,y)\n",
    "            landmamrk 1: eye-2    (x,y)\n",
    "            landmark 2: nose      (x,y)\n",
    "            landmark 3: lip-left  (x,y)\n",
    "            landmar 4: lip-right  (x,y)\n",
    "        \"\"\"\n",
    "        \n",
    "        # extracting landmarks\n",
    "        im, faces, landmarks = self.detector.infer(im_path, resize=[224,224])\n",
    "        \n",
    "        if np.array(landmarks).any() == False:\n",
    "            landmarks = self.get_another_landmark()\n",
    "            with open(\"not_found.txt\",\"a\") as file:\n",
    "                file.write(f\"{im_path}\\n\")\n",
    "\n",
    "        patches = []\n",
    "        \n",
    "        half = self.im_size//2 # for 96, half = 48\n",
    "        \n",
    "        middle_point_x = (landmarks[0][0][0]+landmarks[0][1][0]) //2\n",
    "        middle_point_x = (middle_point_x+landmarks[0][2][0]) // 2\n",
    "        middle_point_y = (landmarks[0][0][1]+landmarks[0][1][1])//2 \n",
    "        middle_point_y = (middle_point_y+ landmarks[0][2][1]) //2\n",
    "        \n",
    "        #middle_point_x -=10\n",
    "        middle_point_y -=15\n",
    "        \n",
    "\n",
    "        if middle_point_y - half < 0:\n",
    "            y1 = 0\n",
    "            y2 = self.im_size\n",
    "        elif middle_point_y + half > im.shape[0]:\n",
    "            y2 = im.shape[0]\n",
    "            y1 = im.shape[0] - self.im_size\n",
    "        else:\n",
    "            y1 = int(middle_point_y) - half\n",
    "            y2 = int(middle_point_y) + half\n",
    "        if middle_point_x - half < 0:\n",
    "            x1 = 0\n",
    "            x2 = self.im_size\n",
    "        elif middle_point_x + half > im.shape[1]:\n",
    "            x2 = im.shape[0]\n",
    "            x1 = im.shape[0] - self.im_size\n",
    "        else:\n",
    "            x1 = int(middle_point_x) - half\n",
    "            x2 = int(middle_point_x) + half\n",
    "        \n",
    "        return y1,y2, x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data(model, dataloader, batch_size, is_keras_model=False, keras_model_name=None):\n",
    "    \n",
    "    if is_keras_model:\n",
    "        if keras_model_name == None:\n",
    "            keras_model = laod_keras_model()\n",
    "        else:\n",
    "            keras_model = laod_keras_model(keras_model_name)\n",
    "    \n",
    "    middle_accuracy = 0\n",
    "    \n",
    "    #model = model.double()\n",
    "    model = model.eval()\n",
    "    \n",
    "    total_steps = 0\n",
    "    \n",
    "    tq = tqdm(dataloader)\n",
    "\n",
    "    for idx, (imgs, label) in enumerate(tq):\n",
    "\n",
    "        if is_keras_model:\n",
    "            preds, correct    = infer_keras_model(keras_model, imgs, label)\n",
    "            running_corrects = correct \n",
    "        else:\n",
    "            img = imgs.cuda(0)\n",
    "            outputs = model(img)\n",
    "            probs, preds = torch.max(outputs, 1)\n",
    "            label = label.reshape(-1)\n",
    "            \n",
    "            running_corrects = torch.sum(probs.cpu() == label.data)\n",
    "            running_corrects = np.sum(preds.cpu().detach().numpy() == label.data.detach().numpy())\n",
    "        \n",
    "        middle_accuracy += running_corrects #.item()\n",
    "        \n",
    "        total_steps += img.size(0)\n",
    "        \n",
    "        temp_acc = middle_accuracy/total_steps\n",
    "        \n",
    "        tq.set_postfix(iter=idx, acc=temp_acc)\n",
    "    \n",
    "    total_accuracy = middle_accuracy/total_steps#(batch_size*total_steps)\n",
    "    print(f\"Total num of acc: {middle_accuracy}| Total iter {total_steps}\")\n",
    "    print(f\"Total eval acc: {total_accuracy}\")\n",
    "    \n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oulu(hard_protocol='Protocol_4'):\n",
    "    \n",
    "    oulu_train_path = \"/home/ec2-user/SageMaker/dataset/spoof-data/oulu\"\n",
    "    \n",
    "    test_imgs = glob.glob(f\"{oulu_train_path}/{hard_protocol}/Test/**/*\") \n",
    "\n",
    "    print(\"OULU data test vs val: \",len(test_imgs))\n",
    "    return test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(im_size, model_name=\"oulu\", modeltype=\"patch\"):\n",
    "    if modeltype == \"patch\":\n",
    "        from models.patch_based_cnn.model import PatchModel\n",
    "        model = PatchModel(im_size)\n",
    "        model = torch.nn.DataParallel(model, device_ids)\n",
    "        model_path = f\"../ckpts/patch_based_cnn/{im_size}/{model_name}_model.pth\"\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "    elif modeltype == \"deeppix\":\n",
    "        model_path = f'../ckpts/deeppix/six_channel/proto4/proto4-best.pt'\n",
    "        model = torch.load(model_path)\n",
    "        model = torch.nn.DataParallel(model.module, device_ids)\n",
    "        \n",
    "    model = model.cuda(device_ids[0])\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OULU data test vs val:  47839\n"
     ]
    }
   ],
   "source": [
    "oulu_model = load_model(48, model_name=\"oulu_Protocol_4\")\n",
    "images = oulu(\"Protocol_4\")\n",
    "dataset = New_PatchDataset(images, transform=transform, im_size=image_size, color_mode=['rgb'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
